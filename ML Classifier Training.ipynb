{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines & Logistic Regression with SIFT and SURF features extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code inspired from <br> \n",
    "https://www.learnopencv.com <br>\n",
    "https://github.com/hugos94 <br>\n",
    "https://pysource.com/2018/03/21/feature-detection-sift-surf-obr-opencv-3-4-with-python-3-tutorial-25 <br>\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html <br>\n",
    "https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.cluster.vq import vq, kmeans\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, training set and validation set paths\n",
    "root = Path('./')\n",
    "data_path = Path('dataset') \n",
    "train_path = data_path.joinpath('train')\n",
    "val_path = data_path.joinpath('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detecting keypoints and descriptors for SURF & SIFT\n",
    "# The function returns descriptors and the labels of the dataset.\n",
    "# SURF descriptors is an array of 64 \n",
    "# SIFT descriptors is an array of 128\n",
    "\n",
    "def detectAndCompute(train_path,featureType):\n",
    "    descriptors = []\n",
    "    labels = []\n",
    "    if featureType == 'SURF':\n",
    "        detector = cv2.xfeatures2d.SURF_create()\n",
    "    elif featureType == 'SIFT':\n",
    "        detector = cv2.xfeatures2d.SIFT_create()\n",
    "    else:\n",
    "        raise ValueError('please select a valid feature type, options: SURF, SIFT')\n",
    "    for child in train_path.iterdir():\n",
    "        for image_path in child.glob('*'):\n",
    "            student_id = str(child.absolute()).split('\\\\')[-1]\n",
    "            image = cv2.imread(str(image_path.absolute()),0)\n",
    "            image = cv2.resize(image,(224,224))\n",
    "            kp, des  = detector.detectAndCompute(image, None)\n",
    "            descriptors.append((str(image_path.absolute()),des))\n",
    "            labels.append(student_id)\n",
    "    return descriptors,labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical stack for descriptors\n",
    "\n",
    "def stackDescriptors(descriptors,featureType):\n",
    "    if featureType == 'SURF':\n",
    "        elementsinarray = 64\n",
    "    elif featureType == 'SIFT':\n",
    "        elementsinarray = 128\n",
    "    else:\n",
    "        raise ValueError('please select a valid feature type, options: SURF, SIFT')\n",
    "    \n",
    "    VstackDescriptors = np.array([], dtype=np.float32).reshape(0,elementsinarray)\n",
    "    for _, descriptor in descriptors:\n",
    "        VstackDescriptors = np.vstack((VstackDescriptors,descriptor))\n",
    "    return VstackDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabulary with kmeans, k=750 centroids which is voc size\n",
    "# This will create our codebook or vocabulary from data set\n",
    "\n",
    "def createVocabulary(VstackDescriptors,vocabularySize=750):\n",
    "    vocabulary, _ = kmeans(VstackDescriptors, vocabularySize, 1)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates image features which is called bag of features\n",
    "\n",
    "def bagOfFeatures(descriptors,vocabulary,vocabularySize):\n",
    "    set_size = len(descriptors)\n",
    "    imageFeatures = np.zeros((set_size, vocabularySize), \"float32\")\n",
    "    for i,(_ , descriptor) in enumerate(descriptors):\n",
    "        words, _ = vq(descriptor, vocabulary)\n",
    "        for word in words:\n",
    "            imageFeatures[i][word] +=1          \n",
    "    return imageFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF & SIFT feature extraction || SVM & LR trainings & evaluation steps:\n",
    "\n",
    "Since we are trying 4 combinations of our feature extractors: SURF & SIFT and classifiers SVM & LR, the steps we will be repetitive. The main steps are explained below:\n",
    "\n",
    "#### Training models\n",
    "1- Extract image features and descriptors on the training set with featureType set to SURF or SIFT <br>\n",
    "2- Stack the descriptors into NumPy array vertically  with featureType set to SURF or SIFT  <br>\n",
    "3- Create a vocabulary of k=750 using kmeans clustering <br>\n",
    "4- Create image features of the training set from the descriptors and vocabulary <br>\n",
    "5- Standarize the features by scaling which happens independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using transform <br>\n",
    "\n",
    "####  Evaluating models on validation Set\n",
    "1- Create image features and descriptors on the validation set with featureType set to SURF or SIFT <br>\n",
    "2- Create image features of the validation set from the descriptors and vocabulary <br> \n",
    "3- Standarize the features by scaling the validation data <br>\n",
    "4- Predict <br>\n",
    "5- Display accuracy <br>\n",
    "\n",
    "Note: Training and validation accuracy is displayed below for every combination. A Classification report will be printed only for  best combo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****************************************************     SURF    *****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureType = 'SURF'\n",
    "vocabularySize = 750\n",
    "descriptors,labels = detectAndCompute(train_path,featureType)\n",
    "VstackDescriptors =  stackDescriptors(descriptors,featureType)\n",
    "vocabulary = createVocabulary(VstackDescriptors,vocabularySize)\n",
    "imageFeatures =  bagOfFeatures(descriptors,vocabulary,vocabularySize)\n",
    "imageFeaturesScaler = StandardScaler().fit(imageFeatures)\n",
    "imageFeaturesTransformed = imageFeaturesScaler.transform(imageFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SURF vocabulary\n",
    "np.save('vocabulary/SURFvocabulary750.npy', vocabulary, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machines grid search parameters\n",
    "\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate classfier to support vector machines\n",
    "svm = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "# Start training with grid search cv set to 5 and utlizing all workers\n",
    "svmGrid = GridSearchCV(svm, param_grid, cv = 3, n_jobs = -1, iid=False)\n",
    "\n",
    "# SVM Grid fit with images features\n",
    "svmGrid.fit(imageFeaturesTransformed, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best paramters from grid search:{'C': 100, 'gamma': 0.001}.\n",
      "SVM accuracy from grid search: 0.866929056809575.\n"
     ]
    }
   ],
   "source": [
    "# Display best score and parameters\n",
    "\n",
    "print('SVM best paramters from grid search:%s.' %svmGrid.best_params_)\n",
    "print('SVM accuracy from grid search: %s.' %svmGrid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/SURF&SVM_K750.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best SVM model\n",
    "svmBest = svmGrid.best_estimator_\n",
    "# Set labels to list and store\n",
    "labelList = list(set(labels))\n",
    "\n",
    "# Persist an arbitrary Python object using joblib and saving all features, vocabulary and best classifier into one compressed pickle file for SURF & SVM \n",
    "joblib.dump((svmBest, labelList, imageFeaturesScaler, vocabularySize, vocabulary), 'models/SURF&SVM_K750.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating SURF & SVM model on validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptorsValidation,labelsValidation = detectAndCompute(val_path,featureType)\n",
    "imageFeaturesValidation =  bagOfFeatures(descriptorsValidation,vocabulary,vocabularySize)\n",
    "imageFeaturesScalerValidation = StandardScaler().fit(imageFeaturesValidation)\n",
    "imageFeaturesTransformedValidation = imageFeaturesScalerValidation.transform(imageFeaturesValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svmBest.predict(imageFeaturesTransformedValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9517857142857142"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy \n",
    "accuracy_score(labelsValidation,list(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   unknown 7       1.00      0.90      0.95        10\n",
      "          42       1.00      1.00      1.00        10\n",
      "           2       0.83      1.00      0.91        10\n",
      "   unknown 3       0.82      0.90      0.86        10\n",
      "   unknown 5       1.00      1.00      1.00        10\n",
      "           4       1.00      1.00      1.00        10\n",
      "           6       1.00      1.00      1.00        10\n",
      "          17       1.00      1.00      1.00        10\n",
      "          11       1.00      1.00      1.00        10\n",
      "          40       1.00      0.90      0.95        10\n",
      "           1       0.91      1.00      0.95        10\n",
      "          22       1.00      1.00      1.00        10\n",
      "          10       1.00      1.00      1.00        10\n",
      "          15       0.91      1.00      0.95        10\n",
      "          33       0.89      0.80      0.84        10\n",
      "          28       1.00      1.00      1.00        10\n",
      "          14       0.90      0.90      0.90        10\n",
      "           5       1.00      1.00      1.00        10\n",
      "          60       0.91      1.00      0.95        10\n",
      "          38       1.00      0.90      0.95        10\n",
      "          25       0.83      1.00      0.91        10\n",
      "          56       0.77      1.00      0.87        10\n",
      "   unknown 1       1.00      1.00      1.00        10\n",
      "          29       1.00      1.00      1.00        10\n",
      "          50       0.86      0.60      0.71        10\n",
      "          13       1.00      0.90      0.95        10\n",
      "   unknown 8       1.00      0.70      0.82        10\n",
      "          12       1.00      1.00      1.00        10\n",
      "           3       1.00      1.00      1.00        10\n",
      "          54       0.89      0.80      0.84        10\n",
      "          58       0.91      1.00      0.95        10\n",
      "          52       0.82      0.90      0.86        10\n",
      "   unknown 4       1.00      1.00      1.00        10\n",
      "          16       0.91      1.00      0.95        10\n",
      "          27       0.91      1.00      0.95        10\n",
      "          78       1.00      1.00      1.00        10\n",
      "          26       1.00      0.90      0.95        10\n",
      "          36       1.00      0.90      0.95        10\n",
      "           8       1.00      0.90      0.95        10\n",
      "          21       0.91      1.00      0.95        10\n",
      "   unknown 6       0.90      0.90      0.90        10\n",
      "          32       1.00      1.00      1.00        10\n",
      "           9       0.90      0.90      0.90        10\n",
      "   unknown 2       0.91      1.00      0.95        10\n",
      "          18       1.00      1.00      1.00        10\n",
      "          31       1.00      1.00      1.00        10\n",
      "          30       1.00      0.90      0.95        10\n",
      "          19       0.90      0.90      0.90        10\n",
      "          44       1.00      1.00      1.00        10\n",
      "          46       1.00      1.00      1.00        10\n",
      "          48       1.00      0.90      0.95        10\n",
      "          23       1.00      1.00      1.00        10\n",
      "          24       1.00      1.00      1.00        10\n",
      "          34       0.91      1.00      0.95        10\n",
      "           7       1.00      1.00      1.00        10\n",
      "          20       1.00      0.90      0.95        10\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       560\n",
      "   macro avg       0.96      0.95      0.95       560\n",
      "weighted avg       0.96      0.95      0.95       560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing classification report for best performing combination (SURF & SVM)\n",
    "\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(labelsValidation, prediction, target_names=labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF & LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitc regression grid search parameters\n",
    "\n",
    "solvers = ['lbfgs', 'liblinear']\n",
    "pen  = ['l2']\n",
    "Cs =  [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 1000, 5000]\n",
    "  \n",
    "param_grid = [{'C':Cs, 'penalty':pen, 'solver':solvers, 'multi_class':['auto']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid=[{'C': array([1.00000e-04, 4.64159e-02, 2.15443e+01, 1.00000e+04]), 'penalty': ['l2'], 'solver': ['lbfgs'], 'multi_class': ['auto']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate classfier to support vector machines\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "# Start training with grid search cv set to 5 and workers 2\n",
    "lrGrid = GridSearchCV(logistic, param_grid, cv = 3, n_jobs = -1, iid=False)\n",
    "# LR Grid fit with images features\n",
    "lrGrid.fit(imageFeaturesTransformed, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR best paramters from grid search:{'C': 0.046415888336127774, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'lbfgs'}.\n",
      "LR accuracy from grid search: 0.8754980695224338.\n"
     ]
    }
   ],
   "source": [
    "# Display best score and parameters\n",
    "\n",
    "print('LR best paramters from grid search:%s.' %lrGrid.best_params_)\n",
    "print('LR accuracy from grid search: %s.' %lrGrid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/SURF&LR_K750.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best Logistic regression model\n",
    "lrBest = lrGrid.best_estimator_\n",
    "# Set labels to list and store\n",
    "labelList = list(set(labels))\n",
    "\n",
    "# Persist an arbitrary Python object using joblib and saving all features, vocabulary and best classifier into one compressed pickle file for SURF & SVM \n",
    "joblib.dump((lrBest, labelList, imageFeaturesScaler, vocabularySize, vocabulary), 'models/SURF&LR_K750.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating SURF & LR model on validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptorsValidation,labelsValidation = detectAndCompute(val_path,featureType)\n",
    "imageFeaturesValidation =  bagOfFeatures(descriptorsValidation,vocabulary,vocabularySize)\n",
    "imageFeaturesScalerValidation = StandardScaler().fit(imageFeaturesValidation)\n",
    "imageFeaturesTransformedValidation = imageFeaturesScalerValidation.transform(imageFeaturesValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lrBest.predict(imageFeaturesTransformedValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9160714285714285"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy \n",
    "accuracy_score(labelsValidation,list(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****************************************************     SIFT    *****************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureType = 'SIFT'\n",
    "vocabularySize = 750\n",
    "descriptors,labels = detectAndCompute(train_path,featureType)\n",
    "VstackDescriptors =  stackDescriptors(descriptors,featureType)\n",
    "vocabulary = createVocabulary(VstackDescriptors,vocabularySize)\n",
    "imageFeatures =  bagOfFeatures(descriptors,vocabulary,vocabularySize)\n",
    "imageFeaturesScaler = StandardScaler().fit(imageFeatures)\n",
    "imageFeaturesTransformed = imageFeaturesScaler.transform(imageFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SIFT vocabulary\n",
    "np.save('vocabulary/SIFTvocabulary750.npy', vocabulary, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machines grid search parameters\n",
    "\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "gammas = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.001, 0.005, 0.01, 0.05, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate classfier to support vector machines with RBF kernel\n",
    "svm = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "# Start training with grid search cv set to 5 and utlizing all workers\n",
    "svmGrid = GridSearchCV(svm, param_grid, cv = 3, n_jobs = -1, iid=False)\n",
    "\n",
    "# SVM Grid fit with images features\n",
    "svmGrid.fit(imageFeaturesTransformed, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 0.001}\n",
      "0.7825437392209477\n"
     ]
    }
   ],
   "source": [
    "# display best score and parameters \n",
    "print(svmGrid.best_params_)\n",
    "print(svmGrid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/SIFT&SVM_K750.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best SVM model\n",
    "svmBest = svmGrid.best_estimator_\n",
    "# Set labels to list and store\n",
    "labelList = list(set(labels))\n",
    "\n",
    "# Persist an arbitrary Python object using joblib and saving all features, vocabulary and best classifier into one compressed pickle file for SURF & SVM \n",
    "joblib.dump((svmBest, labelList, imageFeaturesScaler, vocabularySize, vocabulary), 'models/SIFT&SVM_K750.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating SIFT & SVM model on validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptorsValidation,labelsValidation = detectAndCompute(val_path,featureType)\n",
    "imageFeaturesValidation =  bagOfFeatures(descriptorsValidation,vocabulary,vocabularySize)\n",
    "imageFeaturesScalerValidation = StandardScaler().fit(imageFeaturesValidation)\n",
    "imageFeaturesTransformedValidation = imageFeaturesScalerValidation.transform(imageFeaturesValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svmBest.predict(imageFeaturesTransformedValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8803571428571428"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy \n",
    "accuracy_score(labelsValidation,list(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT & LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitc regression grid search parameters\n",
    "\n",
    "solvers = ['lbfgs', 'liblinear']\n",
    "pen  = ['l2']\n",
    "Cs =  [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 1000, 5000]\n",
    "  \n",
    "param_grid = [{'C':Cs, 'penalty':pen, 'solver':solvers, 'multi_class':['auto']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=10000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid=[{'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 1000, 5000], 'penalty': ['l2'], 'solver': ['lbfgs', 'liblinear'], 'multi_class': ['auto']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate classfier to support vector machines\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "\n",
    "# Start training with grid search cv set to 5 and workers 2\n",
    "lrGrid = GridSearchCV(logistic, param_grid, cv = 3, n_jobs = -1, iid=False)\n",
    "\n",
    "# LR Grid fit with images features\n",
    "lrGrid.fit(imageFeaturesTransformed, np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR best paramters from grid search:{'C': 5000, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'lbfgs'}.\n",
      "LR accuracy from grid search: 0.8227237216059814.\n"
     ]
    }
   ],
   "source": [
    "# Display best score and parameters\n",
    "\n",
    "print('LR best paramters from grid search:%s.' %lrGrid.best_params_)\n",
    "print('LR accuracy from grid search: %s.' %lrGrid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/SIFT&LR_K750.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best Logistic regression model\n",
    "lrBest = lrGrid.best_estimator_\n",
    "# Set labels to list and store\n",
    "labelList = list(set(labels))\n",
    "\n",
    "# Persist an arbitrary Python object using joblib and saving all features, vocabulary and best classifier into one compressed pickle file for SURF & SVM \n",
    "joblib.dump((lrBest, labelList, imageFeaturesScaler, vocabularySize, vocabulary), 'models/SIFT&LR_K750.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating SIFT & LR model on validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptorsValidation,labelsValidation = detectAndCompute(val_path,featureType)\n",
    "imageFeaturesValidation =  bagOfFeatures(descriptorsValidation,vocabulary,vocabularySize)\n",
    "imageFeaturesScalerValidation = StandardScaler().fit(imageFeaturesValidation)\n",
    "imageFeaturesTransformedValidation = imageFeaturesScalerValidation.transform(imageFeaturesValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lrBest.predict(imageFeaturesTransformedValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy \n",
    "accuracy_score(labelsValidation,list(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
